features - columns
policy - course of action
the flooris lava - reachin solution asap

linear algebra
  allows you to take a bunch of equasions and stack them on top of each other and then solve for all of them togather.
  when you pipe your speadsheet into ML - each row of data is algebraic equasion where ML is learning coefficient next to each feature
  the whole combination of rows by columns is called a matrix, so your ML algorithm is working with matrix of features in order to compute matrix of coefficience, averages them all togather, boils down to a single formula, which then can be used to predict the stuff we wanna predict

ML is 3 steps:
  Infere/Predict - shot in the dark
  Error/Loss - vector of number telling how far off was the algorithm prediction from the actual value
  Train/Learn - update coefficience or weights
  Online learning - training algorithm based on real life cases

Types of learning
  Supervised learning
    Vision problems (CNN), Speach problems (RNN)
    YOU Train it on data.
    (linear regression, logistic regression)
  Unsupervised learning
    Clustering
    What is in common between data examples?
    Segments of data population, market segmentation
    User segmentation in advertiseming
  Reinforcement learning
    Planning (DQN)
    taking actions (robot movement, playing games)
    give it a goal & system of reward and punishment. if it reaches a goal, give it a rewards; if it fails, punish
    alogrithm will figure out what are the rules of the game, what are the action he can take and what is the best strategy
    learns from its own behavior and trains itself. Policy?

sigmoid function
	Sigmoid squashing function => (-1,1); S(t) = 1/(1+e^-t)
	this insures that neurons value never gets outside of the range of +1 to -1, which is helpful for keeping computation in neuralnet bounded and stable
ReLU - Rectified linear units
	Have very nice stability properties for neuralnet

		rectifier => y€<0,infinity); y = max(0,x)
		if value is positive keep it, otherwise zero

		softplus => y€(0,infitity); y = ln(1+e^x)
		derivative of softplus is logistic function (sigmoid): f'(x)=e^x/(e^x+1)=1/(1+e^-x)
		smoothens out

Grandien descent
	Calculating the slope of error function by calculating the slope (slope = change in error / change in weight= de/dw) - calculus 

Chaining
	
